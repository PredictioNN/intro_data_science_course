{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Logistic Regression with Breast Cancer Data\n",
    "\n",
    "### Introduction to Data Science\n",
    "#### Last Updated: November 28, 2022\n",
    "---  \n",
    "\n",
    "### SOURCES\n",
    "- [Logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "### OBJECTIVES\n",
    "- Implement logistic regression using `sklearn`\n",
    "- Use the sigmoid function to compute the predicted probability\n",
    "- Compute binary classification metrics\n",
    "\n",
    "### CONCEPTS\n",
    "- logistic regression\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Logistic Regression with `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We worked with the sigmoid function for computing probabilities of a binary outcome.  \n",
    "Logistic regression is a model that does these things:\n",
    "- Use a linear combination of predictors as input, equal to: $\\beta_0 + \\beta_1 X_1 + ... + \\beta_n X_n$\n",
    "- Feed the input into the sigmoid function (a.k.a. logistic function)\n",
    "- Estimate the parameters to minimize error\n",
    "- Output a probability estimate of the outcome\n",
    "\n",
    "Now we will work with the Wisconsin Breast Cancer Dataset to predict if a cell is benign ('B') or malignant ('M'). \n",
    "\n",
    "The dataset was sourced here:  \n",
    "https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../datasets/wdbc.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>...</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "      <th>f27</th>\n",
       "      <th>f28</th>\n",
       "      <th>f29</th>\n",
       "      <th>f30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis     f1     f2      f3      f4       f5       f6      f7  \\\n",
       "0    842302         M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001   \n",
       "1    842517         M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869   \n",
       "2  84300903         M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974   \n",
       "3  84348301         M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414   \n",
       "4  84358402         M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980   \n",
       "\n",
       "        f8  ...    f21    f22     f23     f24     f25     f26     f27     f28  \\\n",
       "0  0.14710  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
       "1  0.07017  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.12790  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.10520  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.10430  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "\n",
       "      f29      f30  \n",
       "0  0.4601  0.11890  \n",
       "1  0.2750  0.08902  \n",
       "2  0.3613  0.08758  \n",
       "3  0.6638  0.17300  \n",
       "4  0.2364  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(datapath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About the Fields**\n",
    "\n",
    "`id` - unique identifier for each subject  \n",
    "`diagnosis` - target variable indicating if cell is malignant or benign  \n",
    "`f1-f30` - cell measurement variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**\n",
    "\n",
    "The `diagnosis` field is the target variable. It needs to be converted to values of 0 and 1.  \n",
    "We can make malignant = 1, benign = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['diagnosis'].apply(lambda x: 1 if x == 'M' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure this is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id diagnosis     f1     f2      f3      f4       f5       f6      f7  \\\n",
      "0    842302         M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001   \n",
      "1    842517         M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869   \n",
      "2  84300903         M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974   \n",
      "3  84348301         M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414   \n",
      "4  84358402         M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980   \n",
      "\n",
      "        f8  ...    f22     f23     f24     f25     f26     f27     f28  \\\n",
      "0  0.14710  ...  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
      "1  0.07017  ...  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
      "2  0.12790  ...  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
      "3  0.10520  ...  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
      "4  0.10430  ...  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
      "\n",
      "      f29      f30  target  \n",
      "0  0.4601  0.11890       1  \n",
      "1  0.2750  0.08902       1  \n",
      "2  0.3613  0.08758       1  \n",
      "3  0.6638  0.17300       1  \n",
      "4  0.2364  0.07678       1  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "         id diagnosis     f1     f2      f3      f4       f5       f6  \\\n",
      "564  926424         M  21.56  22.39  142.00  1479.0  0.11100  0.11590   \n",
      "565  926682         M  20.13  28.25  131.20  1261.0  0.09780  0.10340   \n",
      "566  926954         M  16.60  28.08  108.30   858.1  0.08455  0.10230   \n",
      "567  927241         M  20.60  29.33  140.10  1265.0  0.11780  0.27700   \n",
      "568   92751         B   7.76  24.54   47.92   181.0  0.05263  0.04362   \n",
      "\n",
      "          f7       f8  ...    f22     f23     f24      f25      f26     f27  \\\n",
      "564  0.24390  0.13890  ...  26.40  166.10  2027.0  0.14100  0.21130  0.4107   \n",
      "565  0.14400  0.09791  ...  38.25  155.00  1731.0  0.11660  0.19220  0.3215   \n",
      "566  0.09251  0.05302  ...  34.12  126.70  1124.0  0.11390  0.30940  0.3403   \n",
      "567  0.35140  0.15200  ...  39.42  184.60  1821.0  0.16500  0.86810  0.9387   \n",
      "568  0.00000  0.00000  ...  30.37   59.16   268.6  0.08996  0.06444  0.0000   \n",
      "\n",
      "        f28     f29      f30  target  \n",
      "564  0.2216  0.2060  0.07115       1  \n",
      "565  0.1628  0.2572  0.06637       1  \n",
      "566  0.1418  0.2218  0.07820       1  \n",
      "567  0.2650  0.4087  0.12400       1  \n",
      "568  0.0000  0.2871  0.07039       0  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's fit a Logistic Regression model to the data, using two predictors\n",
    "\n",
    "**Import the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare the data, using f1 and f2 as predictors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['f1','f2']].values\n",
    "y = df['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the logistic regression model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract the model coefficients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0462619 , 0.21688595]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract the model intercept**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-19.67135103])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the coefficient on f1 and f2 is 1.0462619 and 0.21688595, respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict the Cell Types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0\n",
      " 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0\n",
      " 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1\n",
      " 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0\n",
      " 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0\n",
      " 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1\n",
      " 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0\n",
      " 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1\n",
      " 0 0 0 1 0 1 0 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# store the predictions in a new column\n",
    "df['label_predicted'] = model.predict(X)\n",
    "\n",
    "# print the predictions\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter the dataframe to show subjects where the prediction matches the target. These are correct predictions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>...</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "      <th>f27</th>\n",
       "      <th>f28</th>\n",
       "      <th>f29</th>\n",
       "      <th>f30</th>\n",
       "      <th>target</th>\n",
       "      <th>label_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>844359</td>\n",
       "      <td>M</td>\n",
       "      <td>18.25</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.11270</td>\n",
       "      <td>0.07400</td>\n",
       "      <td>...</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.14420</td>\n",
       "      <td>0.25760</td>\n",
       "      <td>0.3784</td>\n",
       "      <td>0.1932</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>507 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id diagnosis     f1     f2      f3      f4       f5       f6  \\\n",
       "0      842302         M  17.99  10.38  122.80  1001.0  0.11840  0.27760   \n",
       "1      842517         M  20.57  17.77  132.90  1326.0  0.08474  0.07864   \n",
       "2    84300903         M  19.69  21.25  130.00  1203.0  0.10960  0.15990   \n",
       "4    84358402         M  20.29  14.34  135.10  1297.0  0.10030  0.13280   \n",
       "6      844359         M  18.25  19.98  119.60  1040.0  0.09463  0.10900   \n",
       "..        ...       ...    ...    ...     ...     ...      ...      ...   \n",
       "564    926424         M  21.56  22.39  142.00  1479.0  0.11100  0.11590   \n",
       "565    926682         M  20.13  28.25  131.20  1261.0  0.09780  0.10340   \n",
       "566    926954         M  16.60  28.08  108.30   858.1  0.08455  0.10230   \n",
       "567    927241         M  20.60  29.33  140.10  1265.0  0.11780  0.27700   \n",
       "568     92751         B   7.76  24.54   47.92   181.0  0.05263  0.04362   \n",
       "\n",
       "          f7       f8  ...     f23     f24      f25      f26     f27     f28  \\\n",
       "0    0.30010  0.14710  ...  184.60  2019.0  0.16220  0.66560  0.7119  0.2654   \n",
       "1    0.08690  0.07017  ...  158.80  1956.0  0.12380  0.18660  0.2416  0.1860   \n",
       "2    0.19740  0.12790  ...  152.50  1709.0  0.14440  0.42450  0.4504  0.2430   \n",
       "4    0.19800  0.10430  ...  152.20  1575.0  0.13740  0.20500  0.4000  0.1625   \n",
       "6    0.11270  0.07400  ...  153.20  1606.0  0.14420  0.25760  0.3784  0.1932   \n",
       "..       ...      ...  ...     ...     ...      ...      ...     ...     ...   \n",
       "564  0.24390  0.13890  ...  166.10  2027.0  0.14100  0.21130  0.4107  0.2216   \n",
       "565  0.14400  0.09791  ...  155.00  1731.0  0.11660  0.19220  0.3215  0.1628   \n",
       "566  0.09251  0.05302  ...  126.70  1124.0  0.11390  0.30940  0.3403  0.1418   \n",
       "567  0.35140  0.15200  ...  184.60  1821.0  0.16500  0.86810  0.9387  0.2650   \n",
       "568  0.00000  0.00000  ...   59.16   268.6  0.08996  0.06444  0.0000  0.0000   \n",
       "\n",
       "        f29      f30  target  label_predicted  \n",
       "0    0.4601  0.11890       1                1  \n",
       "1    0.2750  0.08902       1                1  \n",
       "2    0.3613  0.08758       1                1  \n",
       "4    0.2364  0.07678       1                1  \n",
       "6    0.3063  0.08368       1                1  \n",
       "..      ...      ...     ...              ...  \n",
       "564  0.2060  0.07115       1                1  \n",
       "565  0.2572  0.06637       1                1  \n",
       "566  0.2218  0.07820       1                1  \n",
       "567  0.4087  0.12400       1                1  \n",
       "568  0.2871  0.07039       0                0  \n",
       "\n",
       "[507 rows x 34 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['label_predicted'] == df['target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict the Probability of Each Cell Type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.97470913e-01, 8.02529087e-01],\n",
       "       [3.32048070e-03, 9.96679519e-01],\n",
       "       [3.91751573e-03, 9.96082484e-01],\n",
       "       ...,\n",
       "       [2.21665676e-02, 9.77833432e-01],\n",
       "       [2.63048459e-04, 9.99736952e-01],\n",
       "       [9.98034375e-01, 1.96562522e-03]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for the first subject, the probability of a benign cell is 1.97470913e-01  \n",
    "The probability of a malignant cell is 8.02529087e-01\n",
    "\n",
    "Since the malignant probability is greater than the default threshold of 0.5, the predicted cell type is 1 (malignant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting Positive Probabilities**\n",
    "\n",
    "It can be useful to extract the probabilities of the positive label for each subject.  \n",
    "This can be done by extracting the second index across all rows like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.02529087e-01, 9.96679519e-01, 9.96082484e-01, 3.54716872e-02,\n",
       "       9.90691245e-01, 3.76777057e-02, 9.77163862e-01, 3.08025631e-01,\n",
       "       2.07922344e-01, 1.94506527e-01, 8.93803446e-01, 6.72332033e-01,\n",
       "       9.96872102e-01, 8.91515811e-01, 4.00739770e-01, 8.19702945e-01,\n",
       "       5.13418767e-01, 8.44231392e-01, 9.97154370e-01, 8.39013335e-02,\n",
       "       7.05043968e-02, 8.84488084e-04, 3.70773543e-01, 9.99427316e-01,\n",
       "       9.15748188e-01, 8.60386958e-01, 5.62841469e-01, 9.85102078e-01,\n",
       "       8.60225499e-01, 8.78211788e-01, 9.94864935e-01, 3.81312901e-02,\n",
       "       9.65674945e-01, 9.98036551e-01, 7.47018639e-01, 9.25919913e-01,\n",
       "       4.87163824e-01, 1.14708874e-01, 8.14215994e-01, 2.58807657e-01,\n",
       "       2.83088636e-01, 2.70073344e-02, 9.96535802e-01, 2.01242651e-01,\n",
       "       2.38340531e-01, 9.74879417e-01, 5.84698707e-04, 1.36462218e-01,\n",
       "       2.00189562e-02, 3.27234193e-01, 6.40094626e-02, 1.35120621e-01,\n",
       "       3.83098927e-02, 9.69150763e-01, 7.11572266e-01, 2.78736966e-02,\n",
       "       9.88512165e-01, 5.99103075e-01, 1.38290052e-01, 3.04181869e-04,\n",
       "       3.00758875e-03, 2.18201119e-03, 5.10474710e-01, 8.51251545e-04,\n",
       "       2.25449102e-01, 7.28029831e-01, 5.42278498e-03, 2.39248867e-02,\n",
       "       1.55299039e-03, 6.15907459e-02, 9.91563279e-01, 7.48294872e-04,\n",
       "       9.74478900e-01, 1.40842797e-01, 3.88317581e-02, 8.02799932e-01,\n",
       "       4.13800765e-02, 9.37986559e-01, 9.98695943e-01, 9.00949790e-02,\n",
       "       4.13460298e-02, 9.32649423e-02, 9.99994540e-01, 9.97562114e-01,\n",
       "       2.36157346e-02, 9.74897663e-01, 5.33174665e-01, 9.96172757e-01,\n",
       "       1.18007007e-01, 2.59468122e-01, 6.97326403e-01, 7.93477978e-01,\n",
       "       7.00306322e-02, 1.63814543e-01, 5.95368002e-01, 9.98529672e-01,\n",
       "       4.48461094e-02, 6.01850405e-03, 8.57835875e-03, 4.26433102e-01,\n",
       "       4.96521207e-01, 7.83208548e-05, 7.74591273e-02, 5.87661177e-03,\n",
       "       1.08537279e-02, 7.04293955e-02, 2.88227793e-02, 6.18911772e-02,\n",
       "       9.99627555e-01, 3.93261352e-02, 3.15013129e-03, 1.24062335e-01,\n",
       "       3.79929541e-01, 1.34370540e-02, 8.17562743e-04, 7.44753936e-02,\n",
       "       1.01770765e-03, 3.78023092e-01, 8.59061756e-01, 9.69199456e-01,\n",
       "       4.55576021e-03, 9.72481261e-01, 9.99958159e-01, 1.05395159e-01,\n",
       "       1.06398485e-01, 1.90307031e-01, 4.80806355e-01, 9.86730627e-01,\n",
       "       4.21154583e-01, 9.98472123e-01, 1.73782624e-02, 6.74541090e-01,\n",
       "       8.70790328e-01, 4.46862617e-01, 9.87680213e-01, 1.91985658e-01,\n",
       "       2.17929402e-02, 1.24353020e-02, 4.45455582e-01, 6.92674903e-03,\n",
       "       1.02020828e-03, 7.50020138e-01, 1.87380797e-02, 6.17021827e-02,\n",
       "       5.61181846e-03, 1.72345371e-02, 2.34423068e-02, 5.10301986e-01,\n",
       "       2.19086053e-01, 1.96031450e-01, 1.73208544e-01, 1.38235252e-03,\n",
       "       2.10119443e-03, 5.65995933e-03, 7.00464222e-02, 4.90925847e-02,\n",
       "       9.65268998e-01, 8.97366510e-01, 1.35155668e-02, 4.25136677e-03,\n",
       "       4.73800347e-02, 9.79438861e-01, 9.91518916e-01, 1.25508044e-01,\n",
       "       9.99921735e-01, 5.68775289e-01, 1.89666474e-03, 8.76790253e-01,\n",
       "       9.81285958e-01, 4.17609737e-01, 1.63959767e-02, 2.03820916e-01,\n",
       "       2.85494482e-01, 7.47731008e-03, 5.31244263e-03, 5.72335882e-04,\n",
       "       4.53126390e-03, 8.71209613e-01, 2.24387639e-01, 3.11829145e-02,\n",
       "       9.99998697e-01, 9.99713353e-01, 7.61317192e-01, 1.10134412e-02,\n",
       "       7.64217935e-01, 2.87769364e-03, 9.71124058e-01, 2.43314163e-02,\n",
       "       2.81036087e-02, 3.37695468e-02, 5.55000656e-01, 1.58816785e-01,\n",
       "       3.87231376e-03, 2.81927131e-01, 7.13005042e-01, 6.77130182e-02,\n",
       "       3.94141445e-01, 9.81690453e-01, 9.94901602e-01, 4.58239378e-01,\n",
       "       6.70271278e-02, 9.46359563e-01, 9.99971920e-01, 4.82149405e-01,\n",
       "       6.97597069e-02, 4.41712870e-01, 3.71058573e-03, 9.25509539e-01,\n",
       "       2.56117251e-01, 2.90092651e-01, 9.98723778e-01, 4.00870167e-02,\n",
       "       9.99998927e-01, 9.83667586e-01, 5.83986448e-01, 1.82681919e-01,\n",
       "       3.72751865e-02, 5.44112670e-03, 9.96733277e-01, 9.99592243e-01,\n",
       "       7.33959966e-02, 7.80367951e-02, 5.38684457e-03, 7.68385020e-01,\n",
       "       1.09480686e-01, 1.48488999e-01, 4.51727015e-03, 3.51256352e-01,\n",
       "       2.19467967e-01, 1.97063842e-01, 9.09339637e-01, 1.24074957e-01,\n",
       "       3.54485927e-01, 9.99598170e-01, 2.00299268e-03, 4.05290247e-01,\n",
       "       9.99971392e-01, 9.98358440e-01, 7.76739721e-01, 9.99188377e-01,\n",
       "       1.17438074e-01, 3.18357017e-02, 1.97735967e-02, 4.67577874e-01,\n",
       "       9.96740608e-01, 1.21383687e-02, 1.11019274e-01, 4.20968286e-02,\n",
       "       4.48391550e-02, 1.23666693e-02, 9.99356025e-01, 2.56358512e-02,\n",
       "       9.94883097e-01, 8.94106591e-01, 9.92391476e-01, 2.03003131e-01,\n",
       "       9.99109537e-01, 5.25727695e-01, 8.51303895e-01, 9.79281436e-01,\n",
       "       9.99417246e-01, 9.70191014e-01, 9.61513587e-01, 7.03505508e-01,\n",
       "       9.56916007e-01, 9.99844257e-01, 1.13067299e-02, 3.28304732e-01,\n",
       "       6.35517918e-02, 1.72323982e-02, 2.54976060e-01, 6.49077566e-03,\n",
       "       9.99518095e-01, 2.28280555e-03, 9.87841492e-01, 3.02906636e-02,\n",
       "       8.61072660e-03, 9.87159365e-01, 1.70313601e-01, 1.31439007e-01,\n",
       "       9.97858689e-01, 1.27738981e-02, 9.89739260e-01, 8.00734549e-01,\n",
       "       5.84186466e-02, 7.45593372e-02, 6.43784872e-02, 3.42409728e-02,\n",
       "       2.76148484e-02, 2.46413933e-02, 4.21758888e-01, 5.30773245e-01,\n",
       "       6.61280446e-02, 2.97040213e-02, 3.31112231e-02, 8.42273828e-02,\n",
       "       3.76553706e-03, 3.12799046e-02, 3.07711843e-01, 2.49104542e-02,\n",
       "       9.92318951e-01, 8.93932392e-02, 9.98532827e-01, 9.37939776e-03,\n",
       "       2.31454241e-02, 9.76885019e-02, 8.09463344e-02, 7.98612193e-04,\n",
       "       5.78563324e-02, 4.67113043e-02, 3.60755982e-02, 2.72391252e-01,\n",
       "       3.16368698e-02, 5.10472324e-03, 1.30198984e-03, 4.97832499e-02,\n",
       "       2.03497534e-02, 9.70234240e-01, 2.21142044e-03, 4.83719406e-02,\n",
       "       4.32955401e-03, 9.96615864e-01, 3.46396640e-02, 9.98120434e-01,\n",
       "       2.63875646e-02, 6.51870446e-02, 1.07637389e-01, 3.93144435e-02,\n",
       "       8.63307638e-01, 8.89567625e-01, 6.13993094e-01, 1.30774477e-01,\n",
       "       2.59589392e-02, 9.05510517e-03, 6.43346944e-02, 9.38916345e-01,\n",
       "       4.76921388e-02, 9.90193461e-01, 4.70502469e-03, 9.99962461e-01,\n",
       "       2.69539401e-01, 2.55129482e-03, 7.72851115e-03, 9.96392660e-01,\n",
       "       1.68115253e-02, 3.18436949e-03, 4.95342420e-02, 2.62768918e-01,\n",
       "       1.48609598e-02, 1.93806842e-02, 2.25393799e-02, 7.26278419e-01,\n",
       "       9.99983886e-01, 8.44082462e-01, 6.93367101e-03, 8.36094272e-02,\n",
       "       1.20710051e-01, 1.61924621e-01, 8.90288392e-04, 2.94272253e-03,\n",
       "       6.71010262e-02, 2.53915432e-01, 9.66583903e-02, 8.26183171e-01,\n",
       "       1.21828818e-01, 9.98402981e-01, 9.99312794e-01, 4.79614346e-02,\n",
       "       9.98869854e-01, 9.99698571e-01, 9.23176152e-01, 2.86270009e-01,\n",
       "       9.97432479e-01, 9.96619906e-01, 1.34392343e-01, 6.75257430e-01,\n",
       "       1.43871424e-02, 6.29410096e-01, 1.09721282e-01, 1.80781689e-02,\n",
       "       6.24855166e-03, 7.52095833e-03, 1.05623474e-01, 5.13195827e-02,\n",
       "       5.72530129e-02, 6.58214299e-01, 2.10296069e-02, 1.61872832e-01,\n",
       "       1.07904245e-02, 9.97032289e-01, 1.85806929e-03, 1.02612393e-03,\n",
       "       7.04008220e-01, 9.99578213e-01, 4.03737952e-02, 2.25341024e-01,\n",
       "       1.91641835e-01, 7.63948114e-02, 7.51526247e-03, 2.70667614e-02,\n",
       "       9.74071511e-01, 7.97670816e-03, 1.04803820e-01, 6.75041181e-02,\n",
       "       2.88034377e-02, 1.48713467e-02, 6.07839902e-01, 1.69098097e-01,\n",
       "       9.74213124e-01, 4.98727489e-02, 1.84307953e-02, 1.13129584e-02,\n",
       "       5.83783435e-03, 6.91568694e-01, 9.32387893e-01, 6.66191431e-02,\n",
       "       5.91205245e-03, 7.53555225e-01, 2.31051653e-02, 3.38447739e-02,\n",
       "       3.11712326e-02, 2.19308947e-01, 1.71284544e-02, 2.26114025e-01,\n",
       "       4.81205142e-03, 1.03331516e-02, 4.24578091e-03, 2.64789542e-02,\n",
       "       1.18698659e-02, 7.37457898e-02, 6.90913160e-01, 5.40050451e-02,\n",
       "       9.96598616e-01, 9.91709191e-01, 3.89396171e-01, 3.12317445e-01,\n",
       "       1.22602923e-01, 1.80051831e-01, 2.82993906e-01, 1.67099449e-01,\n",
       "       1.13916283e-02, 9.80369268e-01, 1.38329695e-01, 9.57468300e-03,\n",
       "       9.45177881e-01, 1.50794988e-01, 9.93166837e-01, 4.11814609e-01,\n",
       "       4.31780304e-01, 9.98947228e-01, 7.04040794e-02, 9.98067620e-01,\n",
       "       2.70217825e-01, 1.91994840e-01, 6.02035662e-02, 7.29160948e-01,\n",
       "       2.40354767e-01, 4.07608411e-01, 3.49871916e-01, 3.39317852e-02,\n",
       "       9.83491863e-01, 9.99999593e-01, 7.77052617e-01, 2.78497376e-02,\n",
       "       1.25601757e-01, 1.89551172e-01, 1.93839097e-01, 3.57412207e-03,\n",
       "       9.78178742e-01, 2.73634826e-02, 3.88431821e-03, 2.74643978e-01,\n",
       "       3.05123791e-01, 4.17353899e-01, 7.38961255e-03, 5.53993304e-02,\n",
       "       4.10532771e-01, 1.78922313e-01, 1.11463341e-02, 8.26618635e-01,\n",
       "       4.57229997e-02, 2.77795156e-01, 7.38646049e-02, 1.80687770e-01,\n",
       "       3.17089934e-01, 4.36762582e-02, 3.31912351e-01, 9.91420593e-01,\n",
       "       1.90031196e-02, 8.97693584e-01, 1.20498082e-01, 8.66900254e-01,\n",
       "       9.74193940e-01, 2.07898594e-02, 1.90354729e-01, 5.67050826e-01,\n",
       "       7.61869052e-02, 5.36480593e-02, 9.69935178e-01, 9.98465231e-01,\n",
       "       4.24359798e-01, 5.24860854e-01, 4.69024209e-02, 9.99847422e-01,\n",
       "       7.58602695e-04, 1.23205443e-03, 7.31258372e-02, 1.22899558e-02,\n",
       "       6.87364655e-01, 8.45309514e-01, 1.47423101e-02, 2.71309525e-01,\n",
       "       2.31307726e-01, 1.89357504e-01, 5.52531859e-01, 2.25217739e-02,\n",
       "       9.81094527e-01, 9.96061102e-01, 9.58825181e-02, 6.24141488e-02,\n",
       "       9.75453750e-04, 9.99979247e-01, 2.68677442e-02, 2.18288064e-01,\n",
       "       2.55272389e-03, 3.84710657e-04, 1.79159230e-01, 1.63139135e-02,\n",
       "       9.70823215e-02, 1.58591463e-02, 2.74046258e-02, 4.23133572e-02,\n",
       "       1.39825439e-01, 9.98031794e-01, 1.23363738e-02, 9.98262505e-01,\n",
       "       5.37326573e-01, 1.05282891e-01, 2.33741431e-03, 2.22233493e-03,\n",
       "       1.13664748e-02, 7.08485263e-01, 7.79677909e-01, 5.58626240e-01,\n",
       "       3.38461197e-01, 4.05402767e-01, 4.83107810e-03, 4.76949100e-03,\n",
       "       4.74540088e-03, 4.31077380e-02, 2.53268653e-02, 4.07172030e-02,\n",
       "       5.18083885e-01, 5.77680562e-03, 5.19201571e-01, 5.13149443e-02,\n",
       "       8.22323589e-03, 2.26287640e-02, 6.25411929e-01, 8.02617727e-02,\n",
       "       7.14447244e-01, 1.70331846e-01, 9.47539248e-01, 9.99528027e-01,\n",
       "       9.99566043e-01, 9.99456496e-01, 9.77833432e-01, 9.99736952e-01,\n",
       "       1.96562522e-03])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to change the threshold, predicting cell type 1 if the probability of the positive label is greater than 0.85. This now requires a higher confidence compared to using 0.5 as the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True, False,  True, False,  True, False, False,\n",
       "       False,  True, False,  True,  True, False, False, False, False,\n",
       "        True, False, False, False, False,  True,  True,  True, False,\n",
       "        True,  True,  True,  True, False,  True,  True, False,  True,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "        True, False, False, False, False, False, False, False,  True,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "        True, False, False, False, False,  True,  True, False, False,\n",
       "       False,  True,  True, False,  True, False,  True, False, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False,  True,  True, False,  True,  True, False, False, False,\n",
       "       False,  True, False,  True, False, False,  True, False,  True,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True,  True, False, False, False,  True,\n",
       "        True, False,  True, False, False,  True,  True, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "        True,  True, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "        True, False, False,  True,  True, False, False, False, False,\n",
       "        True, False, False,  True, False,  True,  True, False, False,\n",
       "       False, False,  True,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False,  True, False, False,  True,\n",
       "       False, False,  True,  True, False,  True, False, False, False,\n",
       "       False,  True, False, False, False, False, False,  True, False,\n",
       "        True,  True,  True, False,  True, False,  True,  True,  True,\n",
       "        True,  True, False,  True,  True, False, False, False, False,\n",
       "       False, False,  True, False,  True, False, False,  True, False,\n",
       "       False,  True, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True, False, False, False,  True, False,  True,\n",
       "       False, False, False, False,  True,  True, False, False, False,\n",
       "       False, False,  True, False,  True, False,  True, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False,  True,  True, False,  True,\n",
       "        True,  True, False,  True,  True, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True, False, False, False,  True, False, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True,  True, False, False, False, False, False, False, False,\n",
       "        True, False, False,  True, False,  True, False, False,  True,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False,  True,  True, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False,  True, False,  True,  True, False, False,\n",
       "       False, False, False,  True,  True, False, False, False,  True,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True,  True, False, False, False,  True,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True, False,  True, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False,  True,  True,  True,  True,  True,\n",
       "        True, False])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X)[:,1] > 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first subject had a positive probability of 0.8025. When the threshold was 0.5, the system predicted the label to be positive. When we raised the threshold to 0.85, it predicted the label to be false. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**THINK ABOUT AND DISCUSS**\n",
    "\n",
    "1) If you raise the threshold for predicting a positive label, what generally happens to the recall? What generally happens to the precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "answer: Since we require a higher confidence to predict the positive label, the precision will likely increase.  However, the recall will likely decrease, producing more false negatives. This is why it is important to measure precision and recall together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**TRY FOR YOURSELF**\n",
    "\n",
    "You will evaluate the model performance.\n",
    "\n",
    "Hint: you can count the number of rows in a dataframe by using the `len()` function like this: `len(df)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Compute the accuracy of the model, where accuracy = #correct / #total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8910369068541301"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# answer\n",
    "len(df[df['label_predicted'] == df['target']]) / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Count the number of true positives (where the model predicted 1 and the target is 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# answer\n",
    "len(df[ (df['label_predicted'] == 1) & (df['target'] == 1) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Compute the recall of the model, where precision = #true_positive / #actual_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8160377358490566"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# answer\n",
    "len(df[ (df['label_predicted'] == 1) & (df['target'] == 1) ]) / len( df[df['target'] == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) As we saw earlier, the first subject has a probability of malignancy = 8.02529087e-01  \n",
    "Compute this by using the intercept, coefficients, f1, f2 values, and the definition of the sigmoid:\n",
    "\n",
    "$sigmoid = 1 / ( 1 + np.exp(-(b0 + b1 * x1 + b2 * x2) ))$\n",
    "\n",
    "Note this version uses two predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.802529072692052"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#answer\n",
    "b0 = -19.67135103\n",
    "b1 = 1.0462619\n",
    "b2 = 0.21688595\n",
    "x1 = 17.99\n",
    "x2 = 10.38\n",
    "\n",
    "1 / ( 1 + np.exp(-(b0 + b1 * x1 + b2 * x2) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
